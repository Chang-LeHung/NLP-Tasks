{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = np.load(\"final set.npz\")\n",
    "datasets.allow_pickle = True\n",
    "my_mapping = datasets[\"use_word2idx\"]\n",
    "dataset = datasets[\"dataset\"]\n",
    "\n",
    "dufu = torch.from_numpy(dataset.item()[\"dufu\"])\n",
    "sushi = torch.from_numpy(dataset.item()[\"sushi\"])\n",
    "\n",
    "label_dufu = torch.full((len(dufu), ), fill_value=0)\n",
    "label_sushi = torch.full((len(sushi), ), fill_value=1)\n",
    "\n",
    "final_label = torch.cat((label_dufu, label_sushi), dim=0)\n",
    "final_dataset = torch.cat((dufu, sushi), dim=0)\n",
    "\n",
    "final_label = final_label.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2456, 3507, 4682],\n",
       "       [   0,    0,    0, ..., 1447, 3507, 4682],\n",
       "       [   0,    0,    0, ..., 3166, 3507, 4682],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 3124, 3507, 4682],\n",
       "       [   0,    0,    0, ...,  216, 3507, 4682],\n",
       "       [   0,    0,    0, ..., 4316, 3507, 4682]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.item()[\"sushi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = datasets[\"word2idx\"]\n",
    "idx2word = datasets[\"idx2word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2300, 1210])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test, label_train, label_test = train_test_split(final_dataset,\n",
    "                                                            final_label, test_size=0.1,\n",
    "                                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([230, 1210])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2070, 1210])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, label):\n",
    "        \n",
    "        self.datasets = dataset\n",
    "        self.labels = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datasets[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, words_num, embedding_size, hidden_size, classes, num_layers,\n",
    "                    batch_size, sequence_length):\n",
    "        super(PoemClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.words_num = words_num\n",
    "        self.sequence_length = sequence_length\n",
    "        self.emb = nn.Embedding(words_num, embedding_size)\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, sequence_length = x.shape # x batch_size, sequence_length\n",
    "        if hidden is None:\n",
    "            h, c = self.init_hidden(x, batch_size)\n",
    "        else:\n",
    "            h. c = hidden\n",
    "        out = self.emb(x) # batch_size, sequence_length, embedding_size\n",
    "        out, hidden = self.LSTM(out, (h, c)) # batch_size, sequence_length, hidden_size\n",
    "        out = out[:, -1, :]# batch_size, last sequence, hidden_size\n",
    "        out = self.fc1(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, ipt, batch_size):\n",
    "        h = ipt.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        c = ipt.data.new(self.num_layers, batch_size, self.hidden_size).fill_(0).float()\n",
    "        h = Variable(h)\n",
    "        c = Variable(c)\n",
    "        return (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1210"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epoch = 40\n",
    "model = PoemClassifier(len(my_mapping), 128, 128, 2, 2, batch_size, 300)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=0.003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.cuda()\n",
    "datasets = MyDataset(dataset_train, label_train)\n",
    "data_loader = DataLoader(dataset=datasets, batch_size=batch_size, shuffle=True, drop_last=True, \n",
    "                                    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "dataset_test = dataset_test.cuda()\n",
    "label_test = label_test.cuda()\n",
    "for e in range(1, 1 + epoch):\n",
    "    for idx, item in enumerate(data_loader):\n",
    "        data, labels = item\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        h = None\n",
    "        if idx == 0:\n",
    "            out, h = model(data)\n",
    "        else:\n",
    "            out, h = model(data)\n",
    "        loss = criterion(out, labels)\n",
    "        p = (torch.sum(torch.max(out, dim=1)[1] == labels).item()) / batch_size\n",
    "        acc = sum(torch.max(model(dataset_test.detach())[0], dim=1)[1] == label_test).item() / len(dataset_test)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch [{e}/{epoch}] step [{idx + 1}/{len(data_loader)}] loss = {loss.item()} accuracy = {p} val_acc = {round(acc, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"PoemClassify.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
